{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Ivan Morrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: NeuralNetwork Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a class named `NeuralNetwork` using code from lecture notes.  To do this, follow these steps.\n",
    "\n",
    "\n",
    "1. Define the `__init__(self, n_inputs, n_hiddens_each_layer, n_outputs)` function with `n_inputs` as the number of input components in each sample (columns of `X`), `n_hiddens_each_layer` as the number of units in each hidden layer, and `n_outputs` as the number of outputs of the output layer. The length of `n_hiddens_each_layer` determines the number of hidden layers in the created neural network. The `__init__` functions must\n",
    " \n",
    "    1. assign these values to member variables `self.n_inputs`, `self.n_hiddens_each_layer`, and `self.n_outputs`,\n",
    "    1. initialize `self.rmse_trace` to an empty list,\n",
    "    2. initialize `self.n_epochs` to 0,\n",
    "    1. initialze `self.X_means` to `None` to indicate that `X` and `T` have not yet been standardized, and\n",
    "    2. build in `self.Ws` a list of two-dimensional numpy arrays as weight matrices, one for each hidden layer, with uniformly distributed random values between -1 and 1 divided by the square root of the number of inputs to the corresponding layer. Append one more weight matrix of all zero values for the weight matrix in the output layer.\n",
    "\n",
    "1. Define `__repr__(self)` and `__str__(self)` functions that return strings as shown in the examples in this notebook.\n",
    "\n",
    "2. Define `_calc_rmse_standardized(self, Y, T` as shown in lecture notes.\n",
    "\n",
    "1. Define the `_forward(self, X)` function that accepts `X` as standardized values, creates `self.Zs` as a list consisting of the input `X` and the outputs of each hidden layer and the output layer for all samples in `X` and returns the output of the network, `Y`, in standardized form.\n",
    "  \n",
    "1. Define  the `_gradients(self, X, T)` function that accepts `X` and `T` as standardized values and returns a list of numpy arrays containing the gradients of the mean square error with respect to the weights in each layer, in the order of the layers from the first hidden layer, second hidden layer, and so on,  to the output layer.\n",
    "  \n",
    "1. Define the `train(self, Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs, learning_rate)` function that\n",
    "\n",
    "    1. if `self.X_means` is `None`, standardizes `Xtrain` and `Ttrain` and saves the standardization parameters (means and stds) in member variables, `self.X_means`, `self.X_stds`, `self.T_means` and `self.T_stds`,\n",
    "    1. standardizes `Xvalidate` and `Tvalidate` using `self.X_means`, `self.X_stds`, `self.T_means` and `self.T_stds`,\n",
    "    1. loops for `n_epochs` as shown in notes `05` and for each loop,\n",
    "\n",
    "        1. uses the `_forward` function to calculate the outputs of all units,\n",
    "        1. uses the `_gradients` function to calculate the gradient of the the mean squared error respect to all weight matrices,\n",
    "        1. updates all weight matrices using the gradients returned from `_gradients` using SGD, meaning the learning rate multiplied by the gradient,\n",
    "        1. calculates the RMSE for train and validation data and appends a list of these two values to the list `self.rmse_trace`, and\n",
    "    1. increments `self.n_epochs` by `n_epochs`.\n",
    "\n",
    "1. Define `use(self, X)` that\n",
    "\n",
    "    1. standardizes `X` using the standardization member variables,\n",
    "    1. calls `_forward` to calculate the outputs of all units,\n",
    "    1. unstandardizes the output of the network and returns it.\n",
    "\n",
    "1. You may choose to define other functions, such as `_add_ones`, to be called by the functions above. Remember to name functions with a leading `_` that are not meant to be called by the users of your `NeuralNetwork` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.564986Z",
     "start_time": "2022-09-16T20:44:32.561771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_inputs, n_hiddens_each_layer, n_outputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hiddens_each_layer = n_hiddens_each_layer\n",
    "        self.n_outputs = n_outputs\n",
    "        self.rmse_trace = []\n",
    "        self.n_epochs = 0\n",
    "        self.X_means = None\n",
    "        self.Ws = self._initialize_w()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'NeuralNetwork({}, {}, {}'.format(self.n_inputs, self.n_hiddens_each_layer, self.n_outputs)\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.n_epochs is not None:\n",
    "            return f'{self.__repr__()} trained for {self.n_epochs} epochs with a final RMSE of {self.rmse_trace[-1][0]}.'   \n",
    "        else:\n",
    "            return f'{self.__repr__()} has not been trained.'\n",
    "    \n",
    "    def _initialize_w(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self: NeuralNetwork class\n",
    "\n",
    "        Iterates over each layer of NN and fetches the number of units in each given layer from self.n_hiddens_each_layer.\n",
    "        If looking at the very first layer, use the model inputs as the units_prev_layer variable. Otherwise,\n",
    "        fetch the units in the previous hidden layer from n_hiddens_each_layer using the index.\n",
    "\n",
    "        Creates a Numpy array of weights for the given layer based on its num of units and the units in the prev\n",
    "        layer. Appends these to a running list W.\n",
    "\n",
    "        Finally, appends an array of all 0s as the final layer to W for the weight matrix in the output layer.\n",
    "        \"\"\"\n",
    "        W = []\n",
    "        if len(self.n_hiddens_each_layer) >= 1:\n",
    "            for layer in range(len(self.n_hiddens_each_layer)):\n",
    "                units_in_layer = self.n_hiddens_each_layer[layer]\n",
    "                if layer == 0:\n",
    "                    units_prev_layer = self.n_inputs\n",
    "                else:\n",
    "                    units_prev_layer = self.n_hiddens_each_layer[layer - 1]\n",
    "                W.append(np.random.uniform(-1, 1, size=(1 + units_prev_layer, units_in_layer)) / np.sqrt(1 + units_in_layer))\n",
    "            \n",
    "            # append 0's for the weight matrix of output layer\n",
    "            W.append(np.zeros(shape=(1 + self.n_hiddens_each_layer[-1], self.n_outputs)))\n",
    "        else:\n",
    "            W.append(np.zeros(shape=(1 + self.n_inputs, self.n_outputs)))\n",
    "\n",
    "        return W\n",
    "\n",
    "    def _calc_rmse_standardized(self, T, Y):\n",
    "        # multiply by std. dev. of T to undo std. dev. operation\n",
    "        error = (T - Y) * self.T_stds\n",
    "        return np.sqrt(np.mean(error**2)) \n",
    "\n",
    "    # Add constant column of 1's\n",
    "    def _add_ones(self, A):\n",
    "        return np.insert(A, 0, 1, axis=1)\n",
    "\n",
    "    # standardize matrix data\n",
    "    def _standardize(self, matrix, means=None, stds=None):\n",
    "        if means is None and stds is None:\n",
    "            means = matrix.mean(axis=0)\n",
    "            stds = matrix.std(axis=0)\n",
    "\n",
    "        standardized_matrix = (matrix - means) / stds\n",
    "        return standardized_matrix, means, stds\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.Zs = []\n",
    "        self.Zs.append(X)\n",
    "        input = X\n",
    "        for i, weights in enumerate(self.Ws):\n",
    "            # add ones to input matrix for mat mul with weights & biases\n",
    "            input = self._add_ones(input)\n",
    "            # apply tanh activation function on all hidden layers\n",
    "            if i < len(self.Ws) - 1:\n",
    "                layer_output = np.tanh(input @ weights)\n",
    "            # no activation function on output layer\n",
    "            else:\n",
    "                layer_output = input @ weights\n",
    "    \n",
    "            self.Zs.append(layer_output)\n",
    "            input = layer_output\n",
    "\n",
    "        # input holds result from output_layer of model\n",
    "        return input\n",
    "    \n",
    "    def _gradients(self, X, T, verbose=False):\n",
    "        gradients = []\n",
    "        # Start with the output layer\n",
    "        error = (self.Zs[-1] - T) / self.n_inputs * self.n_outputs\n",
    "        if verbose:\n",
    "            self._start_gradients_message(error)\n",
    "        \n",
    "        for layer in range(len(self.Ws) - 1, -1, -1):\n",
    "            print('layer: ', layer)\n",
    "            \n",
    "            # Current layer's activations\n",
    "            current_activations = self.Zs[layer]\n",
    "            # Compute gradient for current layer's weights\n",
    "            grad_wrt_W = -current_activations.T @ error\n",
    "            \n",
    "            # Calculate gradient for biases\n",
    "            grad_wrt_b = - np.sum(error, axis=0, keepdims=True)\n",
    "            \n",
    "            full_gradient = np.vstack([grad_wrt_b, grad_wrt_W])\n",
    "            \n",
    "            gradients.insert(0, full_gradient)\n",
    "                    \n",
    "            # If not the input layer, propagate error to previous layer\n",
    "            if layer > 0:\n",
    "                error = error @ self.Ws[layer][1:, :].T\n",
    "                \n",
    "                if self.Zs[layer-1][1].shape == (1,):\n",
    "                    prev_activations = self.Zs[layer-1]\n",
    "                else:\n",
    "                    prev_activations = self.Zs[layer-1][:, 1:]\n",
    "                    \n",
    "                # Apply derivative of activation function\n",
    "                error = error * (1 - prev_activations**2)\n",
    "\n",
    "            if verbose:\n",
    "                self._print_gradients_progress(layer, gradients, error)\n",
    "        print('Returning reversed list of gradients.')\n",
    "        return gradients\n",
    "        \n",
    "    def _start_gradients_message(self, output_error):\n",
    "        print('In _gradients, just before for loop.\\nShapes of self.Zs elements: {}'.format(\n",
    "            ', '.join(str(z.shape) for z in self.Zs)\n",
    "        ))\n",
    "        print('In _gradients, just before for loop.\\nShapes of self.Ws elements: {}'.format(\n",
    "            ', '.join(str(w.shape) for w in self.Ws)\n",
    "        ))\n",
    "        print('delta (backpropped from output layer) is ', output_error)\n",
    "        print('gradients list is empty.\\n')\n",
    "        \n",
    "    def _print_gradients_progress(self, loop, gradients, delta):\n",
    "        index = len(self.Ws) - loop\n",
    "        print('End of Pass {} through _gradients for loop:'.format(index))\n",
    "        print('shapes of elements in gradients list are {}'.format(\n",
    "            ', '.join(str(x.shape) for x in gradients)\n",
    "        ))\n",
    "        print('shapes of updated delta is {}'.format(delta.shape))\n",
    "        for i in range(len(gradients)):\n",
    "            print('element {} of gradients list is: {}'.format(\n",
    "                i+1,\n",
    "                gradients[i]\n",
    "            ))\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    def train(self, Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs, learning_rate, verbose=False):\n",
    "        if self.X_means is None:\n",
    "            XtrainStd, self.X_means, self.X_stds = self._standardize(Xtrain)\n",
    "            TtrainStd, self.T_means, self.T_stds = self._standardize(Ttrain)\n",
    "        XvalidateStd, _, _ = self._standardize(Xvalidate, self.X_means, self.X_stds)\n",
    "        TvalidateStd, _, _ = self._standardize(Tvalidate, self.T_means, self.T_stds)\n",
    "        for epoch in range(n_epochs):\n",
    "            print('Epoch is {}. # printed from _train'.format(epoch))\n",
    "            \n",
    "            # Forward pass on all training data\n",
    "            Y = self._forward(XtrainStd)\n",
    "            \n",
    "            # Calculate Gradients of error w.r.t. all weights\n",
    "            gradients = self._gradients(Y, TtrainStd, verbose=verbose)\n",
    "\n",
    "            # Update weight matrices using gradients\n",
    "            for i, weights in enumerate(self.Ws):\n",
    "                print('Taking SGD step...')\n",
    "                self.Ws[i] += weights - learning_rate * gradients[i]\n",
    "                \n",
    "            # calculate error in predictions\n",
    "            epoch_error = self._calc_rmse_standardized(TtrainStd, Y)\n",
    "            \n",
    "            # validation\n",
    "            val_Y = self._forward(XvalidateStd)\n",
    "            val_error = self._calc_rmse_standardized(TvalidateStd, val_Y)\n",
    "            \n",
    "            # append errors\n",
    "            self.rmse_trace.append([epoch_error, val_error])\n",
    "            \n",
    "            self.n_epochs += n_epochs\n",
    "\n",
    "    def use(self, X):\n",
    "        # standardize input\n",
    "        if self.X_means is None:\n",
    "            X, _, _ = self._standardize(X, self.X_means, self.X_stds)\n",
    "        \n",
    "        # predict with model\n",
    "        Ys = self._forward(X)\n",
    "        \n",
    "        # unstandardize model output\n",
    "        Y = (Ys*self.X_stds) + self.X_means\n",
    "        \n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During development of your `NeuralNetwork` class, you may develop it in a python script file.  \n",
    "# Then, to test it you may import it by uncommenting the last line in this cell which assumes \n",
    "# your python script is in `A2mysolution.py`.\n",
    "\n",
    "# Before you check in your notebook, copy and paste the whole `NeuralNetwork` class definition into the\n",
    "# above cell, and delete this cell.\n",
    "\n",
    "# from A2solution import NeuralNetwork, create_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next code cell, I add a new method to your class that replaces the weights created in your constructor with non-random values to allow you to compare your results with mine, and to allow our grading scripts to work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.594067Z",
     "start_time": "2022-09-16T20:44:32.590678Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_weights_for_testing(self):\n",
    "    # Set weights in hidden layers\n",
    "    for W in self.Ws[:-1]:\n",
    "        n_weights = W.shape[0] * W.shape[1]\n",
    "        W[:] = np.linspace(-0.01, 0.01, n_weights).reshape(W.shape)\n",
    "        for u in range(W.shape[1]):\n",
    "            W[:, u] += (u - W.shape[1]/2) * 0.2\n",
    "            \n",
    "    # Set output layer weights to zero\n",
    "    self.Ws[-1][:] = 0\n",
    "\n",
    "setattr(NeuralNetwork, 'set_weights_for_testing', set_weights_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADE TEST CASE RUNS MANUALLY BUT FAILS IN THE SCRIPT? NOT SURE WHY.\n",
    "\n",
    "n_inputs = 3\n",
    "n_hiddens = [2, 1]\n",
    "n_outputs = 2\n",
    "n_samples = 5\n",
    "\n",
    "X = np.arange(n_samples * n_inputs).reshape(n_samples, n_inputs) * 0.1\n",
    "T = np.hstack((X, X*2))\n",
    "\n",
    "nnet = NeuralNetwork(n_inputs, n_hiddens, n_outputs)\n",
    "nnet.set_weights_for_testing()\n",
    "\n",
    "# Set standardization variables so use() will run\n",
    "nnet.X_means = 0\n",
    "nnet.X_stds = 1\n",
    "nnet.T_means = 0\n",
    "nnet.T_stds = 1\n",
    "\n",
    "Y = nnet.use(X)\n",
    "\n",
    "Y_correct = np.array([[0., 0.],\n",
    "    [0., 0.],\n",
    "    [0., 0.],\n",
    "    [0., 0.],\n",
    "    [0., 0.]])\n",
    "np.allclose(Y, Y_correct, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your implementation, using example shown below and additional tests of your own creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0. # printed from _train\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Zs elements: (6, 6), (6, 5), (6, 4), (6, 3), (6, 2), (6, 1)\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Ws elements: (7, 5), (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "delta (backpropped from output layer) is  [[-0.27553167]\n",
      " [-0.14204938]\n",
      " [-0.0192002 ]\n",
      " [ 0.08322975]\n",
      " [ 0.15708098]\n",
      " [ 0.19647053]]\n",
      "gradients list is empty.\n",
      "\n",
      "layer:  4\n",
      "Current activations shape: (6, 2)\n",
      "Current error shape: (6, 1)\n",
      "Weight gradient shape: (2, 1)\n",
      "Full gradient shape: (3, 1)\n",
      "Shape of prev_activation: (6, 2)\n",
      "shape of error: (6, 2)\n",
      "Propagated error shape: (6, 2)\n",
      "End of Pass 1 through _gradients for loop:\n",
      "shapes of elements in gradients list are (3, 1)\n",
      "shapes of updated delta is (6, 2)\n",
      "element 1 of gradients list is: [[-1.11022302e-16]\n",
      " [-2.02692294e-02]\n",
      " [-2.95650568e-04]]\n",
      "\n",
      "\n",
      "layer:  3\n",
      "Current activations shape: (6, 3)\n",
      "Current error shape: (6, 2)\n",
      "Weight gradient shape: (3, 2)\n",
      "Full gradient shape: (4, 2)\n",
      "Shape of prev_activation: (6, 3)\n",
      "shape of error: (6, 3)\n",
      "Propagated error shape: (6, 3)\n",
      "End of Pass 2 through _gradients for loop:\n",
      "shapes of elements in gradients list are (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 3)\n",
      "element 1 of gradients list is: [[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "element 2 of gradients list is: [[-1.11022302e-16]\n",
      " [-2.02692294e-02]\n",
      " [-2.95650568e-04]]\n",
      "\n",
      "\n",
      "layer:  2\n",
      "Current activations shape: (6, 4)\n",
      "Current error shape: (6, 3)\n",
      "Weight gradient shape: (4, 3)\n",
      "Full gradient shape: (5, 3)\n",
      "Shape of prev_activation: (6, 4)\n",
      "shape of error: (6, 4)\n",
      "Propagated error shape: (6, 4)\n",
      "End of Pass 3 through _gradients for loop:\n",
      "shapes of elements in gradients list are (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 4)\n",
      "element 1 of gradients list is: [[-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "element 2 of gradients list is: [[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "element 3 of gradients list is: [[-1.11022302e-16]\n",
      " [-2.02692294e-02]\n",
      " [-2.95650568e-04]]\n",
      "\n",
      "\n",
      "layer:  1\n",
      "Current activations shape: (6, 5)\n",
      "Current error shape: (6, 4)\n",
      "Weight gradient shape: (5, 4)\n",
      "Full gradient shape: (6, 4)\n",
      "Shape of prev_activation: (6, 5)\n",
      "shape of error: (6, 5)\n",
      "Propagated error shape: (6, 5)\n",
      "End of Pass 4 through _gradients for loop:\n",
      "shapes of elements in gradients list are (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 5)\n",
      "element 1 of gradients list is: [[-0. -0. -0. -0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "element 2 of gradients list is: [[-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "element 3 of gradients list is: [[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "element 4 of gradients list is: [[-1.11022302e-16]\n",
      " [-2.02692294e-02]\n",
      " [-2.95650568e-04]]\n",
      "\n",
      "\n",
      "layer:  0\n",
      "Current activations shape: (6, 6)\n",
      "Current error shape: (6, 5)\n",
      "Weight gradient shape: (6, 5)\n",
      "Full gradient shape: (7, 5)\n",
      "End of Pass 5 through _gradients for loop:\n",
      "shapes of elements in gradients list are (7, 5), (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 5)\n",
      "element 1 of gradients list is: [[-0. -0. -0. -0. -0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "element 2 of gradients list is: [[-0. -0. -0. -0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "element 3 of gradients list is: [[-0. -0. -0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "element 4 of gradients list is: [[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "element 5 of gradients list is: [[-1.11022302e-16]\n",
      " [-2.02692294e-02]\n",
      " [-2.95650568e-04]]\n",
      "\n",
      "\n",
      "Returning reversed list of gradients.\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Epoch is 1. # printed from _train\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Zs elements: (6, 6), (6, 5), (6, 4), (6, 3), (6, 2), (6, 1)\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Ws elements: (7, 5), (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "delta (backpropped from output layer) is  [[-0.27580667]\n",
      " [-0.14232354]\n",
      " [-0.01945841]\n",
      " [ 0.08311668]\n",
      " [ 0.15696844]\n",
      " [ 0.19635821]]\n",
      "gradients list is empty.\n",
      "\n",
      "layer:  4\n",
      "Current activations shape: (6, 2)\n",
      "Current error shape: (6, 1)\n",
      "Weight gradient shape: (2, 1)\n",
      "Full gradient shape: (3, 1)\n",
      "Shape of prev_activation: (6, 2)\n",
      "shape of error: (6, 2)\n",
      "Propagated error shape: (6, 2)\n",
      "End of Pass 1 through _gradients for loop:\n",
      "shapes of elements in gradients list are (3, 1)\n",
      "shapes of updated delta is (6, 2)\n",
      "element 1 of gradients list is: [[ 0.0011453 ]\n",
      " [-0.10472957]\n",
      " [-0.00167858]]\n",
      "\n",
      "\n",
      "layer:  3\n",
      "Current activations shape: (6, 3)\n",
      "Current error shape: (6, 2)\n",
      "Weight gradient shape: (3, 2)\n",
      "Full gradient shape: (4, 2)\n",
      "Shape of prev_activation: (6, 3)\n",
      "shape of error: (6, 3)\n",
      "Propagated error shape: (6, 3)\n",
      "End of Pass 2 through _gradients for loop:\n",
      "shapes of elements in gradients list are (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 3)\n",
      "element 1 of gradients list is: [[ 9.67685508e-05  1.09058143e-06]\n",
      " [ 1.01721140e-03  1.50248512e-05]\n",
      " [ 3.97402210e-04  5.87008792e-06]\n",
      " [-3.44934116e-04 -5.09539047e-06]]\n",
      "element 2 of gradients list is: [[ 0.0011453 ]\n",
      " [-0.10472957]\n",
      " [-0.00167858]]\n",
      "\n",
      "\n",
      "layer:  2\n",
      "Current activations shape: (6, 4)\n",
      "Current error shape: (6, 3)\n",
      "Weight gradient shape: (4, 3)\n",
      "Full gradient shape: (5, 3)\n",
      "Shape of prev_activation: (6, 4)\n",
      "shape of error: (6, 4)\n",
      "Propagated error shape: (6, 4)\n",
      "End of Pass 3 through _gradients for loop:\n",
      "shapes of elements in gradients list are (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 4)\n",
      "element 1 of gradients list is: [[ 2.99995737e-04 -3.67602366e-05  2.30039252e-04]\n",
      " [ 3.69304395e-04  6.62556492e-04  3.97332245e-04]\n",
      " [ 2.76444371e-04  4.93724800e-04  2.97071557e-04]\n",
      " [ 3.31997710e-05  4.94034773e-05  3.40826897e-05]\n",
      " [-2.27215134e-04 -4.26457786e-04 -2.47500923e-04]]\n",
      "element 2 of gradients list is: [[ 9.67685508e-05  1.09058143e-06]\n",
      " [ 1.01721140e-03  1.50248512e-05]\n",
      " [ 3.97402210e-04  5.87008792e-06]\n",
      " [-3.44934116e-04 -5.09539047e-06]]\n",
      "element 3 of gradients list is: [[ 0.0011453 ]\n",
      " [-0.10472957]\n",
      " [-0.00167858]]\n",
      "\n",
      "\n",
      "layer:  1\n",
      "Current activations shape: (6, 5)\n",
      "Current error shape: (6, 4)\n",
      "Weight gradient shape: (5, 4)\n",
      "Full gradient shape: (6, 4)\n",
      "Shape of prev_activation: (6, 5)\n",
      "shape of error: (6, 5)\n",
      "Propagated error shape: (6, 5)\n",
      "End of Pass 4 through _gradients for loop:\n",
      "shapes of elements in gradients list are (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 5)\n",
      "element 1 of gradients list is: [[ 4.19376494e-06 -1.72213854e-05 -2.67969679e-05  2.47805118e-06]\n",
      " [ 2.17350233e-05  2.05688900e-04  1.87596500e-04  1.89094328e-05]\n",
      " [ 1.82093360e-05  1.98269836e-04  1.80516088e-04  1.58943326e-05]\n",
      " [ 9.20648663e-06  1.44057473e-04  1.30088451e-04  8.03299043e-06]\n",
      " [-9.70806813e-06 -1.46211513e-04 -1.31868103e-04 -8.43401424e-06]\n",
      " [-1.86730481e-05 -1.98739644e-04 -1.80946353e-04 -1.62818561e-05]]\n",
      "element 2 of gradients list is: [[ 2.99995737e-04 -3.67602366e-05  2.30039252e-04]\n",
      " [ 3.69304395e-04  6.62556492e-04  3.97332245e-04]\n",
      " [ 2.76444371e-04  4.93724800e-04  2.97071557e-04]\n",
      " [ 3.31997710e-05  4.94034773e-05  3.40826897e-05]\n",
      " [-2.27215134e-04 -4.26457786e-04 -2.47500923e-04]]\n",
      "element 3 of gradients list is: [[ 9.67685508e-05  1.09058143e-06]\n",
      " [ 1.01721140e-03  1.50248512e-05]\n",
      " [ 3.97402210e-04  5.87008792e-06]\n",
      " [-3.44934116e-04 -5.09539047e-06]]\n",
      "element 4 of gradients list is: [[ 0.0011453 ]\n",
      " [-0.10472957]\n",
      " [-0.00167858]]\n",
      "\n",
      "\n",
      "layer:  0\n",
      "Current activations shape: (6, 6)\n",
      "Current error shape: (6, 5)\n",
      "Weight gradient shape: (6, 5)\n",
      "Full gradient shape: (7, 5)\n",
      "End of Pass 5 through _gradients for loop:\n",
      "shapes of elements in gradients list are (7, 5), (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 5)\n",
      "element 1 of gradients list is: [[ 2.27543076e-05  2.20115428e-05  2.12687780e-05  2.05260132e-05\n",
      "   1.97832484e-05]\n",
      " [-1.32473765e-05 -1.28615262e-05 -1.24756760e-05 -1.20898257e-05\n",
      "  -1.17039754e-05]\n",
      " [-1.32473765e-05 -1.28615262e-05 -1.24756760e-05 -1.20898257e-05\n",
      "  -1.17039754e-05]\n",
      " [-1.32473765e-05 -1.28615262e-05 -1.24756760e-05 -1.20898257e-05\n",
      "  -1.17039754e-05]\n",
      " [-1.32473765e-05 -1.28615262e-05 -1.24756760e-05 -1.20898257e-05\n",
      "  -1.17039754e-05]\n",
      " [-1.32473765e-05 -1.28615262e-05 -1.24756760e-05 -1.20898257e-05\n",
      "  -1.17039754e-05]\n",
      " [-1.32473765e-05 -1.28615262e-05 -1.24756760e-05 -1.20898257e-05\n",
      "  -1.17039754e-05]]\n",
      "element 2 of gradients list is: [[ 4.19376494e-06 -1.72213854e-05 -2.67969679e-05  2.47805118e-06]\n",
      " [ 2.17350233e-05  2.05688900e-04  1.87596500e-04  1.89094328e-05]\n",
      " [ 1.82093360e-05  1.98269836e-04  1.80516088e-04  1.58943326e-05]\n",
      " [ 9.20648663e-06  1.44057473e-04  1.30088451e-04  8.03299043e-06]\n",
      " [-9.70806813e-06 -1.46211513e-04 -1.31868103e-04 -8.43401424e-06]\n",
      " [-1.86730481e-05 -1.98739644e-04 -1.80946353e-04 -1.62818561e-05]]\n",
      "element 3 of gradients list is: [[ 2.99995737e-04 -3.67602366e-05  2.30039252e-04]\n",
      " [ 3.69304395e-04  6.62556492e-04  3.97332245e-04]\n",
      " [ 2.76444371e-04  4.93724800e-04  2.97071557e-04]\n",
      " [ 3.31997710e-05  4.94034773e-05  3.40826897e-05]\n",
      " [-2.27215134e-04 -4.26457786e-04 -2.47500923e-04]]\n",
      "element 4 of gradients list is: [[ 9.67685508e-05  1.09058143e-06]\n",
      " [ 1.01721140e-03  1.50248512e-05]\n",
      " [ 3.97402210e-04  5.87008792e-06]\n",
      " [-3.44934116e-04 -5.09539047e-06]]\n",
      "element 5 of gradients list is: [[ 0.0011453 ]\n",
      " [-0.10472957]\n",
      " [-0.00167858]]\n",
      "\n",
      "\n",
      "Returning reversed list of gradients.\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Epoch is 2. # printed from _train\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Zs elements: (6, 6), (6, 5), (6, 4), (6, 3), (6, 2), (6, 1)\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Ws elements: (7, 5), (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "delta (backpropped from output layer) is  [[-0.27926682]\n",
      " [-0.14578395]\n",
      " [-0.0228905 ]\n",
      " [ 0.08290828]\n",
      " [ 0.15674058]\n",
      " [ 0.19613084]]\n",
      "gradients list is empty.\n",
      "\n",
      "layer:  4\n",
      "Current activations shape: (6, 2)\n",
      "Current error shape: (6, 1)\n",
      "Weight gradient shape: (2, 1)\n",
      "Full gradient shape: (3, 1)\n",
      "Shape of prev_activation: (6, 2)\n",
      "shape of error: (6, 2)\n",
      "Propagated error shape: (6, 2)\n",
      "End of Pass 1 through _gradients for loop:\n",
      "shapes of elements in gradients list are (3, 1)\n",
      "shapes of updated delta is (6, 2)\n",
      "element 1 of gradients list is: [[ 0.01216157]\n",
      " [-0.31480873]\n",
      " [-0.0070532 ]]\n",
      "\n",
      "\n",
      "layer:  3\n",
      "Current activations shape: (6, 3)\n",
      "Current error shape: (6, 2)\n",
      "Weight gradient shape: (3, 2)\n",
      "Full gradient shape: (4, 2)\n",
      "Shape of prev_activation: (6, 3)\n",
      "shape of error: (6, 3)\n",
      "Propagated error shape: (6, 3)\n",
      "End of Pass 2 through _gradients for loop:\n",
      "shapes of elements in gradients list are (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 3)\n",
      "element 1 of gradients list is: [[ 3.00599574e-03  3.91660289e-05]\n",
      " [ 1.11578076e-02  1.81884872e-04]\n",
      " [ 5.46673869e-03  8.93086565e-05]\n",
      " [-4.68567326e-03 -7.66922739e-05]]\n",
      "element 2 of gradients list is: [[ 0.01216157]\n",
      " [-0.31480873]\n",
      " [-0.0070532 ]]\n",
      "\n",
      "\n",
      "layer:  2\n",
      "Current activations shape: (6, 4)\n",
      "Current error shape: (6, 3)\n",
      "Weight gradient shape: (4, 3)\n",
      "Full gradient shape: (5, 3)\n",
      "Shape of prev_activation: (6, 4)\n",
      "shape of error: (6, 4)\n",
      "Propagated error shape: (6, 4)\n",
      "End of Pass 3 through _gradients for loop:\n",
      "shapes of elements in gradients list are (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 4)\n",
      "element 1 of gradients list is: [[ 0.00678702 -0.00228423  0.00580168]\n",
      " [ 0.00169834  0.01050395  0.00219981]\n",
      " [ 0.0015975   0.00985463  0.00206733]\n",
      " [ 0.00047557  0.00140607  0.00051221]\n",
      " [-0.00088561 -0.00887262 -0.0013763 ]]\n",
      "element 2 of gradients list is: [[ 3.00599574e-03  3.91660289e-05]\n",
      " [ 1.11578076e-02  1.81884872e-04]\n",
      " [ 5.46673869e-03  8.93086565e-05]\n",
      " [-4.68567326e-03 -7.66922739e-05]]\n",
      "element 3 of gradients list is: [[ 0.01216157]\n",
      " [-0.31480873]\n",
      " [-0.0070532 ]]\n",
      "\n",
      "\n",
      "layer:  1\n",
      "Current activations shape: (6, 5)\n",
      "Current error shape: (6, 4)\n",
      "Weight gradient shape: (5, 4)\n",
      "Full gradient shape: (6, 4)\n",
      "Shape of prev_activation: (6, 5)\n",
      "shape of error: (6, 5)\n",
      "Propagated error shape: (6, 5)\n",
      "End of Pass 4 through _gradients for loop:\n",
      "shapes of elements in gradients list are (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 5)\n",
      "element 1 of gradients list is: [[ 1.37974753e-04 -2.29753164e-04 -3.36931202e-04  1.02584655e-04]\n",
      " [ 1.42656712e-04  1.22205557e-03  1.14110065e-03  1.10071821e-04]\n",
      " [ 1.12932691e-04  1.16854941e-03  1.09260551e-03  8.75366915e-05]\n",
      " [ 4.52087934e-05  9.16202974e-04  8.52596922e-04  3.58365795e-05]\n",
      " [-5.75854285e-05 -9.32148945e-04 -8.65071173e-04 -4.51869074e-05]\n",
      " [-1.19512114e-04 -1.17990987e-03 -1.10284156e-03 -9.25233214e-05]]\n",
      "element 2 of gradients list is: [[ 0.00678702 -0.00228423  0.00580168]\n",
      " [ 0.00169834  0.01050395  0.00219981]\n",
      " [ 0.0015975   0.00985463  0.00206733]\n",
      " [ 0.00047557  0.00140607  0.00051221]\n",
      " [-0.00088561 -0.00887262 -0.0013763 ]]\n",
      "element 3 of gradients list is: [[ 3.00599574e-03  3.91660289e-05]\n",
      " [ 1.11578076e-02  1.81884872e-04]\n",
      " [ 5.46673869e-03  8.93086565e-05]\n",
      " [-4.68567326e-03 -7.66922739e-05]]\n",
      "element 4 of gradients list is: [[ 0.01216157]\n",
      " [-0.31480873]\n",
      " [-0.0070532 ]]\n",
      "\n",
      "\n",
      "layer:  0\n",
      "Current activations shape: (6, 6)\n",
      "Current error shape: (6, 5)\n",
      "Weight gradient shape: (6, 5)\n",
      "Full gradient shape: (7, 5)\n",
      "End of Pass 5 through _gradients for loop:\n",
      "shapes of elements in gradients list are (7, 5), (6, 4), (5, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (6, 5)\n",
      "element 1 of gradients list is: [[0.00015859 0.00015151 0.00014442 0.0001373  0.00013022]\n",
      " [0.00024799 0.00024014 0.00023227 0.00022439 0.00021652]\n",
      " [0.00024799 0.00024014 0.00023227 0.00022439 0.00021652]\n",
      " [0.00024799 0.00024014 0.00023227 0.00022439 0.00021652]\n",
      " [0.00024799 0.00024014 0.00023227 0.00022439 0.00021652]\n",
      " [0.00024799 0.00024014 0.00023227 0.00022439 0.00021652]\n",
      " [0.00024799 0.00024014 0.00023227 0.00022439 0.00021652]]\n",
      "element 2 of gradients list is: [[ 1.37974753e-04 -2.29753164e-04 -3.36931202e-04  1.02584655e-04]\n",
      " [ 1.42656712e-04  1.22205557e-03  1.14110065e-03  1.10071821e-04]\n",
      " [ 1.12932691e-04  1.16854941e-03  1.09260551e-03  8.75366915e-05]\n",
      " [ 4.52087934e-05  9.16202974e-04  8.52596922e-04  3.58365795e-05]\n",
      " [-5.75854285e-05 -9.32148945e-04 -8.65071173e-04 -4.51869074e-05]\n",
      " [-1.19512114e-04 -1.17990987e-03 -1.10284156e-03 -9.25233214e-05]]\n",
      "element 3 of gradients list is: [[ 0.00678702 -0.00228423  0.00580168]\n",
      " [ 0.00169834  0.01050395  0.00219981]\n",
      " [ 0.0015975   0.00985463  0.00206733]\n",
      " [ 0.00047557  0.00140607  0.00051221]\n",
      " [-0.00088561 -0.00887262 -0.0013763 ]]\n",
      "element 4 of gradients list is: [[ 3.00599574e-03  3.91660289e-05]\n",
      " [ 1.11578076e-02  1.81884872e-04]\n",
      " [ 5.46673869e-03  8.93086565e-05]\n",
      " [-4.68567326e-03 -7.66922739e-05]]\n",
      "element 5 of gradients list is: [[ 0.01216157]\n",
      " [-0.31480873]\n",
      " [-0.0070532 ]]\n",
      "\n",
      "\n",
      "Returning reversed list of gradients.\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(0, 6 * 6).reshape(-1, 6)\n",
    "T = np.sin(X[:, 0:1])\n",
    "n_inputs = X.shape[1]\n",
    "n_hiddens_each_layer = [5, 4, 3, 2]\n",
    "n_outputs = T.shape[1]\n",
    "nnet = NeuralNetwork(n_inputs, n_hiddens_each_layer, n_outputs)\n",
    "nnet.set_weights_for_testing()\n",
    "nnet.train(X, T, X, T, n_epochs=3, learning_rate=0.2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test your new `NeuralNetwork` class that allows 0, 1, 2, or more hidden layers with some simple data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.604006Z",
     "start_time": "2022-09-16T20:44:32.595320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1), (100, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(0, 10, 0.1).reshape(-1, 1)\n",
    "T = np.sin(X) + 0.01 * (X ** 2)\n",
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape=(80, 1) Ttrain.shape=(80, 1) Xvalidate.shape=(20, 1) Tvalidate.shape=(20, 1)\n"
     ]
    }
   ],
   "source": [
    "# Collect every 5th sample as the validation set.\n",
    "validate_rows = np.arange(0, X.shape[0], 5)\n",
    "# All remaining samples are in the train set.\n",
    "train_rows = np.setdiff1d(np.arange(X.shape[0]), validate_rows)\n",
    "\n",
    "Xtrain = X[train_rows, :]\n",
    "Ttrain = T[train_rows, :]\n",
    "Xvalidate = X[validate_rows, :]\n",
    "Tvalidate = T[validate_rows, :]\n",
    "\n",
    "print(f'{Xtrain.shape=} {Ttrain.shape=} {Xvalidate.shape=} {Tvalidate.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.692241Z",
     "start_time": "2022-09-16T20:44:32.605307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Xtrain, Ttrain, 'o', label='Train')\n",
    "plt.plot(Xvalidate, Tvalidate, 'o', label='Validate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.697052Z",
     "start_time": "2022-09-16T20:44:32.693526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(1, [3, 2], 1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X.shape[1]\n",
    "n_outputs = T.shape[1]\n",
    "\n",
    "nnet = NeuralNetwork(n_inputs, [3, 2], n_outputs)\n",
    "nnet  # using __repr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.705672Z",
     "start_time": "2022-09-16T20:44:32.698197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, [3, 2], 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.n_inputs, nnet.n_hiddens_each_layer, nnet.n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.714071Z",
     "start_time": "2022-09-16T20:44:32.708215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.rmse_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.723578Z",
     "start_time": "2022-09-16T20:44:32.715275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.4269723 , -0.21070837,  0.04058303],\n",
       "        [-0.35731978,  0.11585473, -0.13737671]]),\n",
       " array([[ 0.45198836, -0.50416932],\n",
       "        [-0.50908213,  0.53812739],\n",
       "        [ 0.26094578, -0.01084677],\n",
       "        [ 0.35922418,  0.45293245]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.732243Z",
     "start_time": "2022-09-16T20:44:32.724790Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet.set_weights_for_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.741699Z",
     "start_time": "2022-09-16T20:44:32.733644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.31 , -0.106,  0.098],\n",
       "        [-0.298, -0.094,  0.11 ]]),\n",
       " array([[-0.21      , -0.00714286],\n",
       "        [-0.20428571, -0.00142857],\n",
       "        [-0.19857143,  0.00428571],\n",
       "        [-0.19285714,  0.01      ]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet.X_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.750374Z",
     "start_time": "2022-09-16T20:44:32.742984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0. # printed from _train\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Zs elements: (80, 1), (80, 3), (80, 2), (80, 1)\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Ws elements: (2, 3), (4, 2), (3, 1)\n",
      "delta (backpropped from output layer) is  [[ 0.56472057]\n",
      " [ 0.43078542]\n",
      " [ 0.2992619 ]\n",
      " [ 0.1714574 ]\n",
      " [-0.06797693]\n",
      " [-0.17724141]\n",
      " [-0.2780798 ]\n",
      " [-0.36950481]\n",
      " [-0.52065815]\n",
      " [-0.57893022]\n",
      " [-0.62489089]\n",
      " [-0.65811469]\n",
      " [-0.68530256]\n",
      " [-0.67907596]\n",
      " [-0.65973309]\n",
      " [-0.62751445]\n",
      " [-0.52606497]\n",
      " [-0.45795576]\n",
      " [-0.37920288]\n",
      " [-0.29065392]\n",
      " [-0.08805168]\n",
      " [ 0.02384227]\n",
      " [ 0.14123522]\n",
      " [ 0.26287997]\n",
      " [ 0.51372308]\n",
      " [ 0.64025312]\n",
      " [ 0.76572495]\n",
      " [ 0.88879718]\n",
      " [ 1.12249669]\n",
      " [ 1.23059994]\n",
      " [ 1.33128073]\n",
      " [ 1.42343184]\n",
      " [ 1.57813906]\n",
      " [ 1.63893341]\n",
      " [ 1.68769214]\n",
      " [ 1.72381334]\n",
      " [ 1.7563581 ]\n",
      " [ 1.7522135 ]\n",
      " [ 1.73430073]\n",
      " [ 1.70267052]\n",
      " [ 1.59913072]\n",
      " [ 1.52798568]\n",
      " [ 1.44464468]\n",
      " [ 1.34979869]\n",
      " [ 1.12890892]\n",
      " [ 1.00477521]\n",
      " [ 0.87293759]\n",
      " [ 0.73455811]\n",
      " [ 0.44312276]\n",
      " [ 0.29265486]\n",
      " [ 0.14079512]\n",
      " [-0.01110785]\n",
      " [-0.3096739 ]\n",
      " [-0.45370479]\n",
      " [-0.59254081]\n",
      " [-0.724977  ]\n",
      " [-0.96617461]\n",
      " [-1.07290404]\n",
      " [-1.16918995]\n",
      " [-1.25426602]\n",
      " [-1.3883034 ]\n",
      " [-1.43633041]\n",
      " [-1.47128835]\n",
      " [-1.49303715]\n",
      " [-1.49702061]\n",
      " [-1.47964743]\n",
      " [-1.44984553]\n",
      " [-1.40813542]\n",
      " [-1.29167457]\n",
      " [-1.21854644]\n",
      " [-1.13673909]\n",
      " [-1.04730615]\n",
      " [-0.85016113]\n",
      " [-0.74490482]\n",
      " [-0.63690993]\n",
      " [-0.52750524]\n",
      " [-0.30985204]\n",
      " [-0.20429122]\n",
      " [-0.1026691 ]\n",
      " [-0.00626429]]\n",
      "gradients list is empty.\n",
      "\n",
      "layer:  2\n",
      "Current activations shape: (80, 2)\n",
      "Current error shape: (80, 1)\n",
      "Weight gradient shape: (2, 1)\n",
      "Full gradient shape: (3, 1)\n",
      "Shape of prev_activation: (80, 2)\n",
      "shape of error: (80, 2)\n",
      "Propagated error shape: (80, 2)\n",
      "End of Pass 1 through _gradients for loop:\n",
      "shapes of elements in gradients list are (3, 1)\n",
      "shapes of updated delta is (80, 2)\n",
      "element 1 of gradients list is: [[1.55431223e-15]\n",
      " [1.16781650e+00]\n",
      " [2.64054414e-02]]\n",
      "\n",
      "\n",
      "layer:  1\n",
      "Current activations shape: (80, 3)\n",
      "Current error shape: (80, 2)\n",
      "Weight gradient shape: (3, 2)\n",
      "Full gradient shape: (4, 2)\n",
      "Shape of previous layer 2nd dim: (1,)\n",
      "Shape of prev_activation: (80, 1)\n",
      "shape of error: (80, 3)\n",
      "Propagated error shape: (80, 3)\n",
      "End of Pass 2 through _gradients for loop:\n",
      "shapes of elements in gradients list are (4, 2), (3, 1)\n",
      "shapes of updated delta is (80, 3)\n",
      "element 1 of gradients list is: [[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "element 2 of gradients list is: [[1.55431223e-15]\n",
      " [1.16781650e+00]\n",
      " [2.64054414e-02]]\n",
      "\n",
      "\n",
      "layer:  0\n",
      "Current activations shape: (80, 1)\n",
      "Current error shape: (80, 3)\n",
      "Weight gradient shape: (1, 3)\n",
      "Full gradient shape: (2, 3)\n",
      "End of Pass 3 through _gradients for loop:\n",
      "shapes of elements in gradients list are (2, 3), (4, 2), (3, 1)\n",
      "shapes of updated delta is (80, 3)\n",
      "element 1 of gradients list is: [[-0. -0. -0.]\n",
      " [ 0.  0.  0.]]\n",
      "element 2 of gradients list is: [[-0. -0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "element 3 of gradients list is: [[1.55431223e-15]\n",
      " [1.16781650e+00]\n",
      " [2.64054414e-02]]\n",
      "\n",
      "\n",
      "Returning reversed list of gradients.\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n",
      "Taking SGD step...\n"
     ]
    }
   ],
   "source": [
    "nnet.train(Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs=1, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.760663Z",
     "start_time": "2022-09-16T20:44:32.751976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.73291748],\n",
       "        [-1.55962573],\n",
       "        [-1.38633399],\n",
       "        [-1.21304224],\n",
       "        [-1.03975049],\n",
       "        [-0.86645874],\n",
       "        [-0.69316699],\n",
       "        [-0.51987524],\n",
       "        [-0.3465835 ],\n",
       "        [-0.17329175],\n",
       "        [ 0.        ],\n",
       "        [ 0.17329175],\n",
       "        [ 0.3465835 ],\n",
       "        [ 0.51987524],\n",
       "        [ 0.69316699],\n",
       "        [ 0.86645874],\n",
       "        [ 1.03975049],\n",
       "        [ 1.21304224],\n",
       "        [ 1.38633399],\n",
       "        [ 1.55962573]]),\n",
       " array([[ 3.90863485e-01,  1.13299913e-01, -1.83151701e-01],\n",
       "        [ 3.00015774e-01,  8.10315818e-02, -1.46065385e-01],\n",
       "        [ 2.03379207e-01,  4.85924892e-02, -1.08563919e-01],\n",
       "        [ 1.02610753e-01,  1.60505622e-02, -7.07508843e-02],\n",
       "        [-3.08708057e-04, -1.65254034e-02, -3.27334092e-02],\n",
       "        [-1.03221630e-01, -4.90663238e-02,  5.37902499e-03],\n",
       "        [-2.03971011e-01, -8.15034121e-02,  4.34758385e-02],\n",
       "        [-3.00577512e-01, -1.13768758e-01,  8.14466324e-02],\n",
       "        [-3.91386449e-01, -1.45795890e-01,  1.19182464e-01],\n",
       "        [-4.75163079e-01, -1.77520319e-01,  1.56577089e-01],\n",
       "        [-5.51128029e-01, -2.08880035e-01,  1.93528131e-01],\n",
       "        [-6.18938061e-01, -2.39815975e-01,  2.29938170e-01],\n",
       "        [-6.78626594e-01, -2.70272430e-01,  2.65715705e-01],\n",
       "        [-7.30521923e-01, -3.00197404e-01,  3.00775985e-01],\n",
       "        [-7.75159652e-01, -3.29542913e-01,  3.35041701e-01],\n",
       "        [-8.13201495e-01, -3.58265220e-01,  3.68443505e-01],\n",
       "        [-8.45367528e-01, -3.86325007e-01,  4.00920385e-01],\n",
       "        [-8.72384518e-01, -4.13687495e-01,  4.32419875e-01],\n",
       "        [-8.94949881e-01, -4.40322491e-01,  4.62898116e-01],\n",
       "        [-9.13709054e-01, -4.66204389e-01,  4.92319779e-01]]),\n",
       " array([[-0.50354771, -0.01809238],\n",
       "        [-0.47647924, -0.01736791],\n",
       "        [-0.44665871, -0.01662004],\n",
       "        [-0.41448009, -0.015855  ],\n",
       "        [-0.38053508, -0.01508   ],\n",
       "        [-0.34557954, -0.01430281],\n",
       "        [-0.3104687 , -0.0135312 ],\n",
       "        [-0.2760723 , -0.01277245],\n",
       "        [-0.24318756, -0.01203292],\n",
       "        [-0.2124684 , -0.01131768],\n",
       "        [-0.184383  , -0.0106305 ],\n",
       "        [-0.15920297, -0.00997379],\n",
       "        [-0.13701888, -0.00934882],\n",
       "        [-0.11777286, -0.00875589],\n",
       "        [-0.10129804, -0.00819461],\n",
       "        [-0.08735731, -0.00766411],\n",
       "        [-0.07567652, -0.00716321],\n",
       "        [-0.06597015, -0.00669058],\n",
       "        [-0.0579595 , -0.00624486],\n",
       "        [-0.05138432, -0.00582469]]),\n",
       " array([[0.05885291],\n",
       "        [0.05568989],\n",
       "        [0.05220543],\n",
       "        [0.04844553],\n",
       "        [0.04447933],\n",
       "        [0.04039512],\n",
       "        [0.03629278],\n",
       "        [0.0322739 ],\n",
       "        [0.02843162],\n",
       "        [0.0248423 ],\n",
       "        [0.02156062],\n",
       "        [0.01861832],\n",
       "        [0.01602598],\n",
       "        [0.01377683],\n",
       "        [0.01185139],\n",
       "        [0.01022197],\n",
       "        [0.00885654],\n",
       "        [0.00772177],\n",
       "        [0.0067851 ],\n",
       "        [0.00601613]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Zs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why only 20 rows in these matrices?  I thought I had 80 training samples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: NeuralNetwork(1, [3, 2], 1 trained for 1 epochs with a final RMSE of 0.740178450121023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.770833Z",
     "start_time": "2022-09-16T20:44:32.761926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(1, [3, 2], 1 trained for 1 epochs with a final RMSE of 0.740178450121023.\n"
     ]
    }
   ],
   "source": [
    "print(nnet)  # using __str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.780487Z",
     "start_time": "2022-09-16T20:44:32.772376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.]), array([2.88530761]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.X_means, nnet.X_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.791055Z",
     "start_time": "2022-09-16T20:44:32.781837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.51792742]), array([0.74017845]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.T_means, nnet.T_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 1), (20, 3), (20, 2), (20, 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for Z in nnet.Zs:\n",
    "    a.append(Z.shape)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.801409Z",
     "start_time": "2022-09-16T20:44:32.792391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 1), (20, 3), (20, 2), (20, 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Z.shape for Z in nnet.Zs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.811612Z",
     "start_time": "2022-09-16T20:44:32.802921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.62 , -0.212,  0.196],\n",
       "        [-0.596, -0.188,  0.22 ]]),\n",
       " array([[-0.42      , -0.01428571],\n",
       "        [-0.40857143, -0.00285714],\n",
       "        [-0.39714286,  0.00857143],\n",
       "        [-0.38571429,  0.02      ]]),\n",
       " array([[-1.55431223e-16],\n",
       "        [-1.16781650e-01],\n",
       "        [-2.64054414e-03]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T20:44:32.822303Z",
     "start_time": "2022-09-16T20:44:32.813047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_means',\n",
       " 'T_stds',\n",
       " 'Ws',\n",
       " 'X_means',\n",
       " 'X_stds',\n",
       " 'Zs',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_ones',\n",
       " '_calc_rmse_standardized',\n",
       " '_forward',\n",
       " '_gradients',\n",
       " '_initialize_w',\n",
       " '_print_gradients_progress',\n",
       " '_standardize',\n",
       " '_start_gradients_message',\n",
       " 'n_epochs',\n",
       " 'n_hiddens_each_layer',\n",
       " 'n_inputs',\n",
       " 'n_outputs',\n",
       " 'rmse_trace',\n",
       " 'set_weights_for_testing',\n",
       " 'train',\n",
       " 'use']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_model(nnet, Xtrain, Ttrain, Xvalidate, Tvalidate):\n",
    "    n_layers = len(nnet.n_hiddens_each_layer)\n",
    "\n",
    "    plt.subplot(2 + n_layers, 1, 1)\n",
    "    plt.plot(nnet.rmse_trace)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(('Train RMSE', 'Validate RMSE'))\n",
    "                   \n",
    "    plt.subplot(2 + n_layers , 1, 2)\n",
    "    plt.plot(Xtrain, nnet.use(Xtrain), '-', label='Ytrain')\n",
    "    plt.plot(Xtrain, Ttrain, 'o', label='Ttrain', alpha=0.5)\n",
    "\n",
    "    plt.plot(Xvalidate, nnet.use(Xvalidate), '-', label='Yvalidate')\n",
    "    plt.plot(Xvalidate, Tvalidate, 'o', label='Tvalidate', alpha=0.5)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('T or Y')\n",
    "    plt.legend()\n",
    "\n",
    "    Xs_for_plotting_Zs = np.linspace(Xtrain.min(), Xtrain.max(), 500).reshape(-1, 1)\n",
    "    nnet.use(Xs_for_plotting_Zs)  # to set nnet.Zs to values for training data.\n",
    "    for layeri, Z in enumerate(nnet.Zs[1:-1][::-1]):  # skip first element (just X) and last element (Y)\n",
    "        plt.subplot(2 + n_layers, 1, layeri + 3)\n",
    "        plt.plot(Z)\n",
    "        plt.title(f'Outputs of Hidden Layer {n_layers - layeri}')\n",
    "        plt.ylabel('Z')\n",
    "        plt.xlabel('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0. # printed from _train\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Zs elements: (90, 1), (90, 10), (90, 5), (90, 1)\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Ws elements: (2, 10), (11, 5), (6, 1)\n",
      "delta (backpropped from output layer) is  [[ 0.5659438 ]\n",
      " [ 0.43194551]\n",
      " [ 0.30035998]\n",
      " [ 0.17249522]\n",
      " [ 0.04961938]\n",
      " [-0.06705199]\n",
      " [-0.17636798]\n",
      " [-0.27725391]\n",
      " [-0.36872202]\n",
      " [-0.51994663]\n",
      " [-0.57824617]\n",
      " [-0.62422851]\n",
      " [-0.65746798]\n",
      " [-0.67766892]\n",
      " [-0.68466866]\n",
      " [-0.67843913]\n",
      " [-0.65908714]\n",
      " [-0.62685331]\n",
      " [-0.52535599]\n",
      " [-0.45721468]\n",
      " [-0.37842466]\n",
      " [-0.28983396]\n",
      " [-0.19239121]\n",
      " [-0.0871362 ]\n",
      " [ 0.0248105 ]\n",
      " [ 0.14225879]\n",
      " [ 0.26396089]\n",
      " [ 0.51492227]\n",
      " [ 0.64151196]\n",
      " [ 0.76704295]\n",
      " [ 0.8901732 ]\n",
      " [ 1.00958194]\n",
      " [ 1.12398289]\n",
      " [ 1.2321371 ]\n",
      " [ 1.33286536]\n",
      " [ 1.42505992]\n",
      " [ 1.57984008]\n",
      " [ 1.64066308]\n",
      " [ 1.6894448 ]\n",
      " [ 1.72558304]\n",
      " [ 1.7485992 ]\n",
      " [ 1.75814314]\n",
      " [ 1.75399658]\n",
      " [ 1.73607536]\n",
      " [ 1.70443025]\n",
      " [ 1.60084163]\n",
      " [ 1.52966305]\n",
      " [ 1.44628275]\n",
      " [ 1.35139205]\n",
      " [ 1.24579455]\n",
      " [ 1.13039814]\n",
      " [ 1.00620591]\n",
      " [ 0.87430613]\n",
      " [ 0.73586141]\n",
      " [ 0.44428867]\n",
      " [ 0.29374982]\n",
      " [ 0.14181849]\n",
      " [-0.0101561 ]\n",
      " [-0.16082697]\n",
      " [-0.30886291]\n",
      " [-0.4529617 ]\n",
      " [-0.59186317]\n",
      " [-0.7243618 ]\n",
      " [-0.96567313]\n",
      " [-1.07245288]\n",
      " [-1.16878418]\n",
      " [-1.25390036]\n",
      " [-1.3271495 ]\n",
      " [-1.38800093]\n",
      " [-1.43605059]\n",
      " [-1.47102501]\n",
      " [-1.49278407]\n",
      " [-1.4967694 ]\n",
      " [-1.47938803]\n",
      " [-1.44957208]\n",
      " [-1.40784231]\n",
      " [-1.35484119]\n",
      " [-1.29132655]\n",
      " [-1.21816394]\n",
      " [-1.13631802]\n",
      " [-1.04684292]\n",
      " [-0.84960495]\n",
      " [-0.74429903]\n",
      " [-0.63625322]\n",
      " [-0.52679695]\n",
      " [-0.4172764 ]\n",
      " [-0.30904113]\n",
      " [-0.20343054]\n",
      " [-0.10176051]\n",
      " [-0.00531025]]\n",
      "gradients list is empty.\n",
      "\n",
      "layer:  2\n",
      "Current activations shape: (90, 5)\n",
      "Current error shape: (90, 1)\n",
      "Weight gradient shape: (5, 1)\n",
      "Full gradient shape: (6, 1)\n",
      "Shape of prev_activation: (90, 9)\n",
      "shape of error: (90, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (90,5) (90,9) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_epochs_per_plot):\n\u001b[1;32m     27\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m---> 28\u001b[0m     nnet\u001b[38;5;241m.\u001b[39mtrain(Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs\u001b[38;5;241m=\u001b[39mn_epochs_per_plot, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m     plot_data_and_model(nnet, Xtrain, Ttrain, Xvalidate, Tvalidate)\n\u001b[1;32m     30\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "Cell \u001b[0;32mIn[2], line 174\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(XtrainStd)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Calculate Gradients of error w.r.t. all weights\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradients(Y, TtrainStd, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Update weight matrices using gradients\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, weights \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs):\n",
      "Cell \u001b[0;32mIn[2], line 128\u001b[0m, in \u001b[0;36mNeuralNetwork._gradients\u001b[0;34m(self, X, T, verbose)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of prev_activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_activations\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m     error \u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m prev_activations\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPropagated error shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (90,5) (90,9) "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(0, 10, 0.1).reshape(-1, 1)\n",
    "T = np.sin(X) + 0.01 * (X ** 2)\n",
    "\n",
    "rows = np.arange(X.shape[0])\n",
    "# Collect every 10th sample as the test set.\n",
    "rows_validate = rows[::10]\n",
    "# All remaining samples are in the train set.\n",
    "rows_train = np.setdiff1d(rows, rows_validate)\n",
    "\n",
    "Xtrain = X[rows_train, :]\n",
    "Ttrain = T[rows_train, :]\n",
    "Xvalidate = X[rows_validate, :]\n",
    "Tvalidate = T[rows_validate, :]\n",
    "\n",
    "n_inputs = X.shape[1]\n",
    "n_hiddens_each_layer = [10, 5]\n",
    "n_outputs = T.shape[1]\n",
    "\n",
    "nnet = NeuralNetwork(n_inputs, n_hiddens_each_layer, n_outputs)\n",
    "nnet.set_weights_for_testing()\n",
    "\n",
    "n_epochs = 10000\n",
    "n_epochs_per_plot = 200\n",
    "\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "for reps in range(n_epochs // n_epochs_per_plot):\n",
    "    plt.clf()\n",
    "    nnet.train(Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs=n_epochs_per_plot, learning_rate=0.2, verbose=True)\n",
    "    plot_data_and_model(nnet, Xtrain, Ttrain, Xvalidate, Tvalidate)\n",
    "    plt.tight_layout()\n",
    "    ipd.clear_output(wait=True)\n",
    "    ipd.display(fig)\n",
    "ipd.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-2, 2, 0.02).reshape(-1, 1)\n",
    "T = np.sin(X) * np.sin(X * 10) + 0.1 * (X + 2)\n",
    "\n",
    "rows = np.arange(X.shape[0])\n",
    "rows_validate = rows[::10]\n",
    "rows_train = np.setdiff1d(rows, rows_validate)\n",
    "\n",
    "Xtrain = X[rows_train, :]\n",
    "Ttrain = T[rows_train, :]\n",
    "Xvalidate = X[rows_validate, :] \n",
    "Tvalidate = T[rows_validate, :] * 0.7\n",
    "\n",
    "n_inputs = X.shape[1]\n",
    "n_hiddens_each_layer = [50, 10, 5]\n",
    "n_outputs = T.shape[1]\n",
    "\n",
    "nnet = NeuralNetwork(n_inputs, n_hiddens_each_layer, n_outputs)\n",
    "nnet.set_weights_for_testing()\n",
    "\n",
    "n_epochs = 50_000\n",
    "n_epochs_per_plot = 500\n",
    "\n",
    "fig = plt.figure(figsize=(8, 10))\n",
    "for reps in range(n_epochs // n_epochs_per_plot):\n",
    "    plt.clf()\n",
    "    nnet.train(Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs=n_epochs_per_plot, learning_rate=0.05)\n",
    "    plot_data_and_model(nnet, Xtrain, Ttrain, Xvalidate, Tvalidate)\n",
    "    plt.tight_layout()\n",
    "    ipd.clear_output(wait=True)\n",
    "    ipd.display(fig)\n",
    "ipd.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to test a neural network with no hidden layers, one hidden layer with a single unit, and other tests you are curious about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of `NeuralNetwork` class to some data related to the energy efficiency of buildings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data from [Energy Efficiency at the UCI ML Repository](https://archive.ics.uci.edu/dataset/242/energy+efficiency). Read it into python using the `pandas.read_csv` function.  Assign the first 8 columns as inputs to `X` and the final two columns as target values to `T`.  Make sure `T` is two-dimensional with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# Read the xlsx file as a pandas.DataFrame\n",
    "data_df = pandas.read_excel('ENB2012_data.xlsx')\n",
    "data = data_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the information at the UCI site, we see these are the names of the input and target components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnames = ['Relative Compactness',\n",
    "    \t  'Surface Area',\n",
    "    \t  'Wall Area',\n",
    "          'Roof Area',\n",
    "    \t  'Overall Height',\n",
    "          'Orientation',\n",
    "    \t  'Glazing Area',\n",
    "    \t  'Glazing Area Distribution']\n",
    "\n",
    "Tnames = ['Heating Load',\n",
    "    \t  'Cooling Load']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Relative Compactness',\n",
       "  'Surface Area',\n",
       "  'Wall Area',\n",
       "  'Roof Area',\n",
       "  'Overall Height',\n",
       "  'Orientation',\n",
       "  'Glazing Area',\n",
       "  'Glazing Area Distribution'],\n",
       " ['Heating Load', 'Cooling Load'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnames, Tnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768, 8), (768, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[:, :8]\n",
    "T = data[:, 8:]\n",
    "X.shape, T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 8), (614, 2), (154, 8), (154, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = np.arange(X.shape[0])\n",
    "np.random.shuffle(rows)\n",
    "train_fraction = 0.8\n",
    "ntrain = round(X.shape[0] * train_fraction)\n",
    "Xtrain = X[rows[:ntrain], :]\n",
    "Ttrain = T[rows[:ntrain], :]\n",
    "Xvalidate = X[rows[ntrain:], :]\n",
    "Tvalidate = T[rows[ntrain:], :]\n",
    "Xtrain.shape, Ttrain.shape, Xvalidate.shape, Tvalidate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function names `create_model` that accepts `n_hiddens_each_layer`, `n_epochs`, and `learning_rate` and that creates and trains a neural network with the correct number of inputs and outputs and the given value of `n_hiddens_each_layer`.  Add a few lines to apply the trained network with the `use` function on the `Xtrain` and `Xvalidate` data and calculates the RMSE for each. Have your function return the neural network and the two output arrays, `Ytrain` and `Yvalidate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(T, Y):\n",
    "    error = (T - Y) \n",
    "    return np.sqrt(np.mean(error ** 2))\n",
    "\n",
    "def create_model(Xtrain, Ttrain, Xvalidate, Tvalidate, n_hiddens_each_layer, n_epochs, learning_rate):\n",
    "    # create and train Neural Network\n",
    "    n_inputs = Xtrain.shape[1]\n",
    "    n_outputs = Ttrain.shape[1]\n",
    "    nnet = NeuralNetwork(n_inputs, n_hiddens_each_layer, n_outputs)\n",
    "    nnet.train(Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs, learning_rate, verbose=True)\n",
    "    \n",
    "    # Apply NN to train and validation data\n",
    "    Ytrain = nnet.use(Xtrain)\n",
    "    Yvalidate = nnet.use(Xvalidate)\n",
    "    \n",
    "    train_rmse = rmse(T=Ttrain, Y=Ytrain)\n",
    "    validate_rmse = rmse(T=Tvalidate, Y=Yvalidate)\n",
    "    \n",
    "    print('Train RMSE: {}'.format(train_rmse))\n",
    "    print('Validation RMSE: {}'.format(validate_rmse))\n",
    "    \n",
    "    return nnet, Ytrain, Yvalidate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch is 0. # printed from _train\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Zs elements: (614, 8), (614, 3), (614, 2), (614, 1), (614, 2)\n",
      "In _gradients, just before for loop.\n",
      "Shapes of self.Ws elements: (9, 3), (4, 2), (3, 1), (2, 2)\n",
      "delta (backpropped from output layer) is  [[ 0.23342329  0.23020425]\n",
      " [ 0.39059655  0.3342755 ]\n",
      " [ 0.0352807  -0.02072899]\n",
      " ...\n",
      " [ 0.12888942  0.20590334]\n",
      " [-0.17180066 -0.13985624]\n",
      " [-0.24504886 -0.34879117]]\n",
      "gradients list is empty.\n",
      "\n",
      "layer:  3\n",
      "Current activations shape: (614, 1)\n",
      "Current error shape: (614, 2)\n",
      "Weight gradient shape: (1, 2)\n",
      "Full gradient shape: (2, 2)\n",
      "Shape of prev_activation: (614, 1)\n",
      "shape of error: (614, 1)\n",
      "Propagated error shape: (614, 1)\n",
      "End of Pass 1 through _gradients for loop:\n",
      "shapes of elements in gradients list are (2, 2)\n",
      "shapes of updated delta is (614, 1)\n",
      "element 1 of gradients list is: [[-4.53637128e-13 -4.43978188e-13]\n",
      " [-1.01510688e+01 -9.37498130e+00]]\n",
      "\n",
      "\n",
      "layer:  2\n",
      "Current activations shape: (614, 2)\n",
      "Current error shape: (614, 1)\n",
      "Weight gradient shape: (2, 1)\n",
      "Full gradient shape: (3, 1)\n",
      "Shape of prev_activation: (614, 2)\n",
      "shape of error: (614, 2)\n",
      "Propagated error shape: (614, 2)\n",
      "End of Pass 2 through _gradients for loop:\n",
      "shapes of elements in gradients list are (3, 1), (2, 2)\n",
      "shapes of updated delta is (614, 2)\n",
      "element 1 of gradients list is: [[-0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "element 2 of gradients list is: [[-4.53637128e-13 -4.43978188e-13]\n",
      " [-1.01510688e+01 -9.37498130e+00]]\n",
      "\n",
      "\n",
      "layer:  1\n",
      "Current activations shape: (614, 3)\n",
      "Current error shape: (614, 2)\n",
      "Weight gradient shape: (3, 2)\n",
      "Full gradient shape: (4, 2)\n",
      "Shape of prev_activation: (614, 7)\n",
      "shape of error: (614, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (614,3) (614,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      3\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 5\u001b[0m nnet, Ytrain, Yvalidate \u001b[38;5;241m=\u001b[39m create_model(Xtrain, Ttrain, Xvalidate, Tvalidate, n_hiddens_each_layer, n_epochs, learning_rate)\n",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(Xtrain, Ttrain, Xvalidate, Tvalidate, n_hiddens_each_layer, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m n_outputs \u001b[38;5;241m=\u001b[39m Ttrain\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m nnet \u001b[38;5;241m=\u001b[39m NeuralNetwork(n_inputs, n_hiddens_each_layer, n_outputs)\n\u001b[0;32m---> 10\u001b[0m nnet\u001b[38;5;241m.\u001b[39mtrain(Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs, learning_rate, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply NN to train and validation data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m Ytrain \u001b[38;5;241m=\u001b[39m nnet\u001b[38;5;241m.\u001b[39muse(Xtrain)\n",
      "Cell \u001b[0;32mIn[6], line 174\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, Xtrain, Ttrain, Xvalidate, Tvalidate, n_epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(XtrainStd)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Calculate Gradients of error w.r.t. all weights\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradients(Y, TtrainStd, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Update weight matrices using gradients\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, weights \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWs):\n",
      "Cell \u001b[0;32mIn[6], line 128\u001b[0m, in \u001b[0;36mNeuralNetwork._gradients\u001b[0;34m(self, X, T, verbose)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of prev_activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_activations\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape of error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m     error \u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m prev_activations\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPropagated error shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (614,3) (614,7) "
     ]
    }
   ],
   "source": [
    "n_hiddens_each_layer = [3, 2, 1]\n",
    "n_epochs=3\n",
    "learning_rate = 0.001\n",
    "\n",
    "nnet, Ytrain, Yvalidate = create_model(Xtrain, Ttrain, Xvalidate, Tvalidate, n_hiddens_each_layer, n_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the following code to plot your results, add another four plots in two rows below these that plot the predicted values on the y axis and the target values on the x axis.  Add labels to all x and y axes that appear in this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(nnet.rmse_trace)\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(Ttrain[:, 0],  label='Ttrain')\n",
    "plt.plot(Ytrain[:, 0],  label='Ytrain')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(Tvalidate[:, 0], label='Tvalidate')\n",
    "plt.plot(Yvalidate[:, 0], label='Yvalidate')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(Ttrain[:, 1], label='Ttrain')\n",
    "plt.plot(Ytrain[:, 1], label='Ytrain')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.plot(Tvalidate[:, 1], label='Tvalidate')\n",
    "plt.plot(Yvalidate[:, 1], label='Yvalidate')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous two code cells several times with different values for `n_hiddens_each_layer`, `n_epochs`, and `learning_rate` until you have values that you think work pretty well.\n",
    "\n",
    "In a markdown cell, write at least four sentences answering each of the following questions.\n",
    "\n",
    "1. How is the training RMSE curve affected by the number of hidden layers and the number of units in each layer?\n",
    "2. How is the final training and validation RMSE affected by the number of epochs and learning rate?\n",
    "3. How much do the final training and validation RMSE values vary for different training runs that differ only in the intial random weights?\n",
    "4. How well does your best model do in predicting heading and cooling load?  In other words, what does an RMSE of a particular value mean in relation to the target values?\n",
    "5. What was the hardest part of this assignment?  What is an estimate of the number of hours you spent on this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I unfortunately have been unable to get my _gradients function to run for any network architecture and have hit the submission time. I have been unable to get create_model function to run for my NeuralNet implementation and have run out of time to debug it. I will try to answer these questions with what I hypothesize would occur.\n",
    "\n",
    "1. I suspect the _training_ RMSE curve will improve very close to 0.0 with the more layers and units that we add. This is because I suspect there will be a large amount of overfitting to the training data. I don't think increasing the number of layers and units exponentially would improve the RMSE of validation or testing data though.\n",
    "\n",
    "2. The more epochs, the lower the training RMSE will get. But the Validation RMSe will stop improving at some point once the model starts overfitting to the training data. Lowering the learning rate will slow down the training time, as the weights will be updated by smaller increments. Increasing the learning rate may prevent us from ever converging on the minimum error that we're looking for. Large learning rates can cause the weight values to jump around sporadically and skip over the minimas.\n",
    "\n",
    "3. I would guess quite a lot. Weight initialization has a large impact on how the model performs. If weights are initialized very poorly it's possible to end up with very poor results from a well designed model.\n",
    "\n",
    "4. \n",
    "\n",
    "5. Definitely the _gradients function, but also making sure the matrix math and dimensions lined up. I probably have spent around 30 hours on this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "\n",
    "Your notebook will be run and graded automatically. Test this grading process by first downloading [A2grader.zip](http://www.cs.colostate.edu/~anderson/cs545/notebooks/A2grader.zip) and unzip `A2grader.py` from it. Run the code in the following cell to demonstrate an example grading session.\n",
    "\n",
    "A different, but similar, grading script will be used to grade your checked-in notebook. It will include additional tests. You should design and perform additional tests on all of your functions to be sure they run correctly before checking in your notebook.  \n",
    "\n",
    "For the grading script to run correctly, you must first name this notebook as `A2solution.ipynb`, and then save this notebook.  Check in your `A2solution.ipynb` notebook when you are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T21:17:11.112089Z",
     "start_time": "2022-09-16T21:17:08.965640Z"
    }
   },
   "outputs": [],
   "source": [
    "%run -i A2grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit (maximum of 2 points)\n",
    "\n",
    "Which inputs does your trained neural network find to be most signficant?  \n",
    "\n",
    "## Extra Credit (up to 1 point)\n",
    "\n",
    "There are many ways to answer this.  For this extra credit, print the absolute values of the weights in the first hidden layer for all units in that layer.  The \"all units\" is the hard part.  Try just taking the mean of the absolute values of the weights for each input across all units.  Do the results make sense to you?\n",
    "\n",
    "## Extra Credit (up to 1 point)\n",
    "\n",
    "Try using a `matplotlib.pyplot` call like\n",
    "\n",
    "```\n",
    "plt.imshow(np.abs(nnet.Ws[0]), interpolation='nearest')\n",
    "plt.colorbar()\n",
    "```\n",
    "\n",
    "to see if you can visually see patterns in the weight magnitudes.  Describe what you see."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
